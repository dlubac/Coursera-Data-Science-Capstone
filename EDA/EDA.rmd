---
title: 'Data Science Capstone: Exploratory Data Analysis'
author: "Matt Dlubac"
date: "January 3, 2018"
output: html_document
---

```{r echo=TRUE}

```


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction
This exploratory data analysis is intended to explore data from blogs, news articles, and tweets. The main goals are to look at the basic features of each dataset and explore the frequency of words and groups of words.

Specifically, the following analyses will be done:

* Line counts and example data from each dataset
* Distribution of word frequency in a combined dataset
* Visualization and word cloud of most commonly occuring words
* Visualizations of most commonly occuring pairs and trios of words

Finally a summary of a plan to create a predictive model and shiny app will be provided.

## Setup
First we will load the libraries needed for the analysis.
```{r echo=TRUE, message=FALSE}
library(quanteda)
library(ggplot2)
```

### Data ingestion
Read in the english language text files
```{r echo=TRUE, warning=FALSE}
blogs <- readLines("../final/en_US/en_US.blogs.txt")
news <- readLines("../final/en_US/en_US.news.txt")
twitter <- readLines("../final/en_US/en_US.twitter.txt")
```

Taking a look at the basic features of each file
```{r echo=TRUE}
# Blogs line count:
length(blogs)

# Blogs sample data:
sample(blogs, 1)

# News line count:
length(news)

# News sample data:
sample(news, 1)

# Twitter line count:
length(twitter)

# Twitter sample data:
sample(twitter, 1)
```

Take a 1% sample of each text file and combine them into a single object
```{r echo=TRUE}
subset_blogs <- sample(blogs, size=length(blogs) * 0.01, replace=FALSE)
subset_news <- sample(news, size=length(news) * 0.01, replace=FALSE)
subset_twitter <- sample(twitter, size=length(twitter) * 0.01, replace=FALSE)

combined_data <- c(subset_blogs, subset_news, subset_twitter)
```

### Clean data
Perform a simple data cleansing by removing all non-alphanumeric characters
```{r echo=TRUE}
combined_data <- gsub("[^0-9A-Za-z///' ]", " ", combined_data)
```

### Transform data to a corpus and Document-Feature Matrix
```{r echo=TRUE}
combined_data <- corpus(combined_data)
dfm_unigram <- dfm(combined_data)
```

### Exploratory Analysis
First get the frequency of all words and look at basic statistics
```{r echo=TRUE}
words <- topfeatures(dfm_unigram, length(dfm_unigram))
summary(words)
```

If we visualize the distribution of word frequency it can be seen that nearly all words appear less than 1000 times.
```{r echo=TRUE}
hist(words, breaks=300, xlab="Word Count")
```

### Find the most frequently occuring words
The topfeatues function can be used to find the 10 most frequently occuring words
```{r echo=TRUE}
top_words_unigram <- topfeatures(dfm_unigram, n=10)
top_words_unigram <- data.frame(keyName=names(top_words_unigram), value=top_words_unigram, row.names=NULL)
top_words_unigram <- top_words_unigram[order(-top_words_unigram$value),]
```

The most common words can be visualized with a wordcloud and bar chart
```{r echo=TRUE}
textplot_wordcloud(dfm_unigram, min.freq=500, random.order=FALSE, rot.per=0.25)

unigram_plot <- ggplot(top_words_unigram, aes(x=reorder(top_words_unigram$keyName, -top_words_unigram$value), y=top_words_unigram$value)) + 
  geom_bar(stat="identity") +
  ggtitle("Most Common Unigrams") +
  xlab("Word") +
  ylab("Occurances")

print(unigram_plot)
```

### Find the most frequently occuring pairs of words
```{r echo=TRUE}
dfm_bigram <- dfm(combined_data, ngrams=2)
top_words_bigram <- topfeatures(dfm_bigram, n=10)
top_words_bigram <- data.frame(keyName=names(top_words_bigram), value=top_words_bigram, row.names=NULL)
top_words_bigram <- top_words_bigram[order(-top_words_bigram$value),]
```

```{r echo=TRUE}
bigram_plot <- ggplot(top_words_bigram, aes(x=reorder(top_words_bigram$keyName, -top_words_bigram$value), y=top_words_bigram$value)) + 
  geom_bar(stat="identity") +
  ggtitle("Most Common Bigrams") +
  xlab("Word") +
  ylab("Occurances")

print(bigram_plot)
```

### Find the most frequently occuring trios of words
```{r echo=TRUE}
dfm_trigram <- dfm(combined_data, ngrams=3)
top_words_trigram <- topfeatures(dfm_trigram, n=10)
top_words_trigram <- data.frame(keyName=names(top_words_trigram), value=top_words_trigram, row.names=NULL)
top_words_trigram <- top_words_trigram[order(-top_words_trigram$value),]
```

```{r echo=TRUE}
trigram_plot <- ggplot(top_words_trigram, aes(x=reorder(top_words_trigram$keyName, -top_words_trigram$value), y=top_words_trigram$value)) + 
  geom_bar(stat="identity") +
  ggtitle("Most Common Trigrams") +
  xlab("Word") +
  ylab("Occurances") +
  theme(axis.text.x = element_text(angle=45, hjust=1))

print(trigram_plot)
```

## Prediction Model & Shiny App Development
The prediction model that will be created as part of this capstone will use parts of this exploratory analysis as its basis. The most common bigrams, trigrams, and quadgrams will first be determined. Those ngrams can then be used to predict the next word in input text by comparing word positions. It will also allow trigrams to be used if a quadgram cannot be found, and bigrams if an trigram cannot be found.

The shiny app will provide a textbox for users to input text data. Depending on the performance of the final prediction model the word prediction will either be done in real time and display the result to the user in a separate textbox or the user will click a button after inputting data to perform the prediction and output. The app will also provide documentation for users.

